{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text 토큰화 시키는 방법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "text='해보지 않으면 해낼 수 없다'\n",
    "print('text: ',text)\n",
    "print('토큰화: ',text_to_word_sequence(text))\n",
    "\n",
    "uimport nujnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 1, 5, 6, 2, 7], [1, 8, 2, 9, 3, 10, 11], [12, 13, 3, 14, 15, 16]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#토크나이저 클래스를 활용하여 단어를 토큰화 한 다음 단어의 개수를 셀 수 있음\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "docs = ['먼저 텍스트의 각 단어를 토큰화 합니다.',\n",
    "        '텍스트의 단어로 토큰화 해서 딥러닝에서 인식 됩니다.',\n",
    "        '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.']\n",
    "token=Tokenizer()\n",
    "token.fit_on_texts(docs) #토큰화 시킴\n",
    "print(token.texts_to_sequences(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'텍스트의': 1, '토큰화': 2, '딥러닝에서': 3, '먼저': 4, '각': 5, '단어를': 6, '합니다': 7, '단어로': 8, '해서': 9, '인식': 10, '됩니다': 11, '토큰화한': 12, '결과는': 13, '사용할': 14, '수': 15, '있습니다': 16}\n",
      "\n",
      " {1: '텍스트의', 2: '토큰화', 3: '딥러닝에서', 4: '먼저', 5: '각', 6: '단어를', 7: '합니다', 8: '단어로', 9: '해서', 10: '인식', 11: '됩니다', 12: '토큰화한', 13: '결과는', 14: '사용할', 15: '수', 16: '있습니다'}\n",
      "\n",
      " OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('토큰화', 2), ('합니다', 1), ('단어로', 1), ('해서', 1), ('딥러닝에서', 2), ('인식', 1), ('됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n",
      "\n",
      " 16\n",
      "\n",
      " 3\n",
      "\n",
      " defaultdict(<class 'int'>, {'텍스트의': 2, '합니다': 1, '각': 1, '토큰화': 2, '단어를': 1, '먼저': 1, '단어로': 1, '딥러닝에서': 2, '해서': 1, '됩니다': 1, '인식': 1, '수': 1, '있습니다': 1, '결과는': 1, '사용할': 1, '토큰화한': 1, 0: 0})\n"
     ]
    }
   ],
   "source": [
    "print('\\n',token.word_index) #단어마다 인덱스 설정\n",
    "\n",
    "print('\\n',token.index_word) #거꾸로\n",
    "\n",
    "print('\\n',token.word_counts) #단어마다의 갯수\n",
    "\n",
    "print('\\n',len(token.word_counts)) #토큰화한 각 단어의 출현 빈도수\n",
    "\n",
    "print('\\n',token.document_count) #문장의 수\n",
    "\n",
    "print('\\n',token.word_docs) # 단어마다 나오는 문장의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['먼저 텍스트의 각 단어를 토큰화 합니다.', '텍스트의 단어로 토큰화 해서 딥러닝에서 인식 됩니다.', '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.']\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "먼저    :1\n",
      "텍스트의    :2\n",
      "각    :1\n",
      "단어를    :1\n",
      "토큰화    :2\n",
      "합니다.    :1\n",
      "단어로    :1\n",
      "해서    :1\n",
      "딥러닝에서    :2\n",
      "인식    :1\n",
      "됩니다.    :1\n",
      "토큰화한    :1\n",
      "결과는    :1\n",
      "사용할    :1\n",
      "수    :1\n",
      "있습니다.    :1\n"
     ]
    }
   ],
   "source": [
    "def count_words(docs):\n",
    "    result = {}\n",
    "    for sentence in docs:\n",
    "        words = sentence.split(' ')\n",
    "        for word in words:\n",
    "            if word in result:\n",
    "                result[word] += 1\n",
    "            else:\n",
    "                result[word] = 1\n",
    "    return result\n",
    "result = count_words(docs)\n",
    "for word ,cnt in result.items():\n",
    "    print(f'{word}    :{cnt}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "먼저   : 1\n",
      "텍스트의 : 1\n",
      "각    : 1\n",
      "단어를  : 1\n",
      "토큰화  : 1\n",
      "합니다. : 1\n",
      "단어로  : 1\n",
      "토큰화해서: 1\n",
      "딥러닝에서: 1\n",
      "인식   : 1\n",
      "됩니다. : 1\n",
      "토큰화한 : 1\n",
      "결과는  : 1\n",
      "사용할  : 1\n",
      "수    : 1\n",
      "있습니다.: 1\n"
     ]
    }
   ],
   "source": [
    "text = \"먼저 텍스트의 각 단어를 토큰화 합니다. 단어로 토큰화해서 딥러닝에서 인식 됩니다. 토큰화한 결과는 사용할 수 있습니다.\"\n",
    "word_count = {}\n",
    "for word in text.split():\n",
    "    word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "max_word_len = max(len(word) for word in word_count.keys())\n",
    "max_count_len = max(len(str(count)) for count in word_count.values())\n",
    "\n",
    "for word, count in word_count.items():\n",
    "    print(\"{:<{}}: {:>{}}\".format(word, max_word_len, count, max_count_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"
     ]
    }
   ],
   "source": [
    "# 단어의 원-핫 인코딩\n",
    " \n",
    "#1. 문장을 토크나이징 \n",
    "# 2. 단어의 인덱스를 확인-> 변수에 추가\n",
    "# 3. 단어의 인덱스 원핫 인코딩 -> 인덱스 갯수 +1 로 to_categorical\n",
    "\n",
    "text='오랫동안 꿈꾸는 이는 그 꿈을 닮아간다.'mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n",
    "token =Tokenizer()\n",
    "token.fit_on_texts([text])\n",
    "print(token.word_index)\n",
    "x_index = token.texts_to_sequences([text]) #단어의 인덱스만 리스트로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_index #에대해 원핫 인코딩\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "to_categorical(x_index,num_classes=len(x_index[0])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#딥러닝을 활용한 자연어 처리\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "docs = [\"너무 재밌네요\",\"최고예요\",\"참 잘 만든 영화예요\",\"추천하고 싶은 영화입니다\",\"한번 더 보고싶네요\",\"글쎄요\",\"별로예요\",\"생각보다 지루하네요\",\"연기가 어색해요\",\"재미없어요\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"
     ]
    }
   ],
   "source": [
    "classes=np.array([1,1,1,1,1,0,0,0,0,0]) \n",
    "\n",
    "#자연어 전처리 -> 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "print(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩한 결과 출력 : [[ 0  0  1  2]\n",
      " [ 0  0  0  3]\n",
      " [ 4  5  6  7]\n",
      " [ 0  8  9 10]\n",
      " [ 0 11 12 13]\n",
      " [ 0  0  0 14]\n",
      " [ 0  0  0 15]\n",
      " [ 0  0 16 17]\n",
      " [ 0  0 18 19]\n",
      " [ 0  0  0 20]]\n"
     ]
    }
   ],
   "source": [
    "x= token.texts_to_sequences(docs)\n",
    "padded_x = pad_sequences(x,4)\n",
    "print('패딩한 결과 출력 :',padded_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 8)              168       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 201\n",
      "Trainable params: 201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#임베딩에 입력된 단어의 수를 지정\n",
    "word_size=len(token.word_index)+1\n",
    "#단어 임베딩을 포함한 딥러닝 모델 생성\n",
    "model =Sequential()\n",
    "model.add(Embedding(word_size, 8,input_length=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 1s 7ms/step - loss: 0.5065 - accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5022 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4983 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4947 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4913 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4877 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4844 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4809 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.4775 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4741 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4706 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4672 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4638 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4603 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4570 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.4534 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4499 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.4467 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4430 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.4396 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c01632f3a0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(padded_x,classes,batch_size=8,epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4366 - accuracy: 1.0000\n",
      "accuracy:1.0000\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:{:.4f}'.format(model.evaluate(padded_x,classes)[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 97ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.73667485]], dtype=float32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded_x[3].reshape(1,-1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
