{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chap18 시퀸스 배열로 다루는 순환 신경망 (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#로이터 뉴스 데이터 로드를 함\n",
    "from tensorflow.keras.datasets import reuters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#num_words= 1000 단어의 빈도수가 1~1000 사이의 단어만 가져옴\n",
    "(X_train,y_train),(X_test,y_test) = reuters.load_data(num_words=1000,test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape,X_test.shape,X_test.shape[0]+X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(np.unique(y_train))) #46개의 카테고리 확인\n",
    "\n",
    "#X_train 학습용 기사 , y_train 뉴스의 카테고리 46개 존재 \n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "X_train=sequence.pad_sequences(X_train, maxlen=100)\n",
    "X_test=sequence.pad_sequences(X_test, maxlen=100)\n",
    "y_train=to_categorical(y_train,46)\n",
    "y_test=to_categorical(y_test,46)\n",
    "# y_train_padded = pad_sequences(y_train, maxlen=100, padding='post', truncating='post')\n",
    "# y_test_padded = pad_sequences(y_test, maxlen=100, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from tensorflow.keras.models import Sequential\n",
    " from tensorflow.keras.layers import Dense,Embedding,LSTM\n",
    " from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=100))# 1000 기사당 단어수 100\n",
    "model.add(LSTM(100,activation='tanh'))#lstm으로 rnn 구현\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "Early_Stopping=EarlyStopping(monitor='val_loss', patience=10, verbose=0)\n",
    "history= model.fit(X_train, y_train, epochs=100,verbose=1, batch_size=20,validation_data=(X_test,y_test), callbacks=[Early_Stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test ,verbose=1)[1]\n",
    "import matplotlib.pyplot as plt\n",
    "y_vloss=history.history['val_loss']\n",
    "y_loss=history.history['loss']\n",
    "x_len=np.arange(len(y_vloss))\n",
    "plt.plot(x_len,y_loss,label='loss',marker='.',c='red')\n",
    "plt.plot(x_len,y_vloss,label='v_loss',marker='.',c='blue')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs=['중부 지방은 대체로 맑으나 남부 지방은 구름이 많겠습니다',\n",
    "           '이번 선거에는 누가 이길 수 있을까?',\n",
    "           '올 초부터 유동성의 힘으로 주가가 일정하게 상승']\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "token=Tokenizer()\n",
    "token.fit_on_texts(test_docs)\n",
    "\n",
    "test_docs_token=token.texts_to_sequences(test_docs)\n",
    "[np.argmax(line) for line in model.predict(sequence.pad_sequences(test_docs_token,maxlen=100))]\n",
    "# np.argmax(model.predict(sequence.pad_sequences(test_docs_token,maxlen=100)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 과 CNN 의 조합을 이용함 영화 리뷰 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "# IMDB(영화리뷰 )데이터셋에서 데이터 가져옴\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.layers import Dense, Dropout, MaxPooling1D, Conv1D\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000)\n",
    "print(len(x_train), len(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어의 수를 맞춤\n",
    "x_train= sequence.pad_sequences(x_train, maxlen=500)\n",
    "x_test= sequence.pad_sequences(x_test, maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델설정 -> dropout(0.5),conv1d(64,커널사이즈 4 추가) #임베딩 단어 압축하는거\n",
    "model=Sequential()\n",
    "model.add(Embedding(5000,500))\n",
    "model.add(Conv1D(64,4,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(4))\n",
    "model.add(LSTM(55))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "early_Stopping=EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "history=model.fit(x_train,y_train,epochs=100,validation_split=0.25,callbacks=[early_Stopping],batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test ,verbose=1)[1]\n",
    "import matplotlib.pyplot as plt\n",
    "y_vloss=history.history['val_loss']\n",
    "y_loss=history.history['loss']\n",
    "x_len=np.arange(len(y_vloss))\n",
    "plt.plot(x_len,y_loss,label='loss',marker='.',c='red')\n",
    "plt.plot(x_len,y_vloss,label='v_loss',marker='.',c='blue')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(x_test)),model.predict(x_test,batch_size=128).flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어 처리를 위한 딥러닝\n",
    "\n",
    "##데이터 전처리 과정 ##\n",
    "# 불필요한 단어 제거\n",
    "# 문장을 단어로 토큰화\n",
    "# 각 문장을 인덱스의 배열로 변환시킴\n",
    "# 각 문장을 같은 사이즈로 패딩\n",
    "#카테고리를 원-핫 인코딩 (옵션)\n",
    "\n",
    "## 모델 설정및 실행 ##\n",
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=1000,output_dim=100) #단어 임베딩 \n",
    "model.add(Conv1D(filters=32,kernel_size=3,activation='relu')) #CNN 추가\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.4)) #과적합 방지를 위한 \n",
    "model.LSTM(units=128,activation='relu') #순서가있는 자료를 처리하기위함 RNN방법\n",
    "model.summary()\n",
    "# 모델 실행 옵션\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,batch_size=128,epochs=100,validation_data=(x_test,y_test),validation_split=0.25,callbacks=[])\n",
    "\n",
    "model.evaluate(x_test,y_test,batch_size=128)\n",
    "model.predict(x_test,batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, None, 500)         2500000   \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, None, 64)          128064    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, None, 64)          0         \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPooling  (None, None, 64)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, None, 55)          26400     \n",
      "                                                                 \n",
      " attention_2 (Attention)     (None, 128)               17105     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,671,698\n",
      "Trainable params: 2,671,698\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from attention import Attention\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM,Embedding,Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "model=Sequential()\n",
    "model.add(Embedding(5000,500))\n",
    "model.add(Conv1D(64,4,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(4))\n",
    "model.add(LSTM(55,return_sequences=True))\n",
    "model.add(Attention())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "37/37 [==============================] - 113s 3s/step - loss: 0.5405 - accuracy: 0.7236 - val_loss: 0.3476 - val_accuracy: 0.8594\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 99s 3s/step - loss: 0.2496 - accuracy: 0.8996 - val_loss: 0.2968 - val_accuracy: 0.8758\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 92s 2s/step - loss: 0.1773 - accuracy: 0.9351 - val_loss: 0.3016 - val_accuracy: 0.8728\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 87s 2s/step - loss: 0.1376 - accuracy: 0.9524 - val_loss: 0.2885 - val_accuracy: 0.8792\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 87s 2s/step - loss: 0.1038 - accuracy: 0.9637 - val_loss: 0.3274 - val_accuracy: 0.8686\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 91s 2s/step - loss: 0.0696 - accuracy: 0.9763 - val_loss: 0.3489 - val_accuracy: 0.8779\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 91s 2s/step - loss: 0.0522 - accuracy: 0.9825 - val_loss: 0.4504 - val_accuracy: 0.8675\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 97s 3s/step - loss: 0.0399 - accuracy: 0.9863 - val_loss: 0.4332 - val_accuracy: 0.8730\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 97s 3s/step - loss: 0.0368 - accuracy: 0.9879 - val_loss: 0.4652 - val_accuracy: 0.8712\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "early_Stopping=EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "history=model.fit(x_train,y_train,epochs=100,validation_split=0.25,callbacks=[early_Stopping],batch_size=512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
